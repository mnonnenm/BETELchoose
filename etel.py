import numpy as np
import torch
import scipy.optimize as spo
from scipy.special import logsumexp

dtype = torch.float64
dtype_np = np.float64

def log_w_opt(洧랝, G):
    洧랙 = torch.bmm(G, 洧랝.unsqueeze(-1)).squeeze(-1)
    return 洧랙 - torch.logsumexp(洧랙,axis=-1).unsqueeze(-1)

def 풙(洧랝, G):
    洧랙 = torch.matmul(G, 洧랝.unsqueeze(-1)).squeeze(-1)
    return torch.logsumexp(洧랙, axis=-1)

def 풙_np(洧랝, G):
    洧랙 = G.dot(洧랝)    
    return logsumexp(洧랙, axis=-1)

def grad풙(洧랝, G):
    w = torch.exp(log_w_opt(洧랝, G))
    return torch.bmm(w.unsqueeze(-2), G).squeeze(-2)

def solve_洧랝(G):
    with torch.no_grad():
        N, K = G.shape[0], G.shape[-1]
        洧랝0 = np.zeros((N, K))
        洧랝 = np.zeros_like(洧랝0)
        for i in range(N):
            def 풙_G(洧랝):
                return 풙_np(洧랝, G[i].numpy())
            洧랝[i] = spo.minimize(풙_G, 洧랝0[i])['x']
    return torch.tensor(洧랝,dtype=dtype)

def log_pX洧랚(g, X, 洧랚, eps=1e-4):
    G = g(X, 洧랚)
    洧랝 = solve_洧랝(G)
    log_w = log_w_opt(洧랝, G)
    log_p = log_w.sum(axis=-1)

    # check for estimating equation constraints to hold
    GTw = torch.bmm(torch.exp(log_w).unsqueeze(-2), G).transpose(-1,-2)
    idx_n_bad = torch.abs(GTw.squeeze(-1)).mean(axis=-1) >= eps
    log_p[idx_n_bad] = - torch.inf # likelihood = 0 if origin not in convex hull !
    
    return log_p

def comp_dFd洧랝(洧랝, G, GTdiagw):

    F = grad풙(洧랝, G) # F(洧랝) = d풙d洧랝
    dFd洧랝 = torch.bmm(GTdiagw, G) - torch.bmm(F.unsqueeze(-1), F.unsqueeze(-2))

    return dFd洧랝

def comp_dFd洧랯(洧랝, G, w, GTw, GTdiagw, dGd洧랯):

    M = torch.eye(洧랝.shape[-1]).unsqueeze(0) - torch.bmm(GTw, 洧랝.unsqueeze(-2))

    dFd洧랯 = {}
    for (p, dGdp) in dGd洧랯.items():
        dFdp = torch.bmm(M, (dGdp.flatten(3) * w.unsqueeze(-1).unsqueeze(-1)).sum(axis=1))
        dFd洧랯[p] = dFdp + torch.bmm(GTdiagw, (dGdp.flatten(3) * 洧랝.unsqueeze(-2).unsqueeze(-1)).sum(axis=-2))

    return dFd洧랯

def grad_log_pX洧랚(g, X, 洧랚, eps=1e-4):

    N,T = X.shape[:2]                               # N x T x D
    G = g(X, 洧랚)                                     # N x T x K
    洧랝 = solve_洧랝(G)                                  # N     x K  
    w = torch.exp(log_w_opt(洧랝, G))                  # N x T

    GTw = torch.bmm(w.unsqueeze(-2), G).transpose(-1,-2)
    idx_n_good = torch.abs(GTw.squeeze(-1)).mean(axis=-1) < eps
    if idx_n_good.sum() < N:
        print(f"warning, {N - idx_n_good.sum()} out of {N} datapoints have zero likelihood and no gradient")
        X, 洧랚, G, 洧랝, w, GTw = X[idx_n_good], 洧랚[idx_n_good], G[idx_n_good], 洧랝[idx_n_good], w[idx_n_good], GTw[idx_n_good]

    GTdiagw = (G  * w.unsqueeze(-1)).transpose(-1,-2)
    dGd洧랯=g.jacobian_pars(X, 洧랚)                      # struct of { par : N x T x K x dim(par) }

    # inverse function theorem 
    dFd洧랝 = comp_dFd洧랝(洧랝, G, GTdiagw)                 # N     x K x K
    dFd洧랯 = comp_dFd洧랯(洧랝, G, w, GTw, GTdiagw, dGd洧랯)  # struct of { par : N x K x dim(par) }
    d洧랝d洧랯 = {}                                       # struct of { par : N x K x dim(par) }
    for (p, dFdp) in dFd洧랯.items():
        d洧랝d洧랯[p] = - torch.linalg.solve(dFd洧랝, dFdp) # Inverse function theorem: d洧랝d洧랯 = inv(dFd洧랝) * dFd洧랯 

    # differentiating w* and g_洧랯(X,洧랚) wrt 洧랯        
    diff  = (1. - T * w).unsqueeze(-1)              # N x T x 1
    grad = {}                                       # struct of { par : N x T x dim(par) }
    for (p, dGdp) in dGd洧랯.items():
        grad[p] = torch.zeros((N, np.prod(dGdp.shape[3:]))) # N x dim(par)
        grad[p][idx_n_good] = torch.bmm(洧랝.unsqueeze(-2), (diff.unsqueeze(-1) * dGdp.flatten(3)).sum(axis=1)).squeeze(-2)
    for (p, d洧랝dp) in d洧랝d洧랯.items():
        grad[p][idx_n_good] = grad[p][idx_n_good] + torch.bmm((diff * G).sum(axis=-2).unsqueeze(-2), d洧랝dp).squeeze(-2)

    return grad

def loss_InfoNCE(g, X, 洧랚):

    N = X.shape[0]
    idx = torch.arange(N)
    idx洧랚, idxX = torch.repeat_interleave(idx, N), idx.repeat(N)
    log_p_all = log_pX洧랚(g, X[idxX], 洧랚[idx洧랚]).reshape(N,N)
    log_normalizers = torch.logsumexp(log_p_all, axis=-1)
    log_p = torch.diag(log_p_all)
    return (log_p - log_normalizers).sum(axis=0)

def grad_InfoNCE(g, X, 洧랚):

    N = X.shape[0]
    assert 洧랚.shape[0] == N # could untie this, but not necessary for basic usage

    idx = torch.arange(N)
    idx洧랚, idxX = torch.repeat_interleave(idx, N), idx.repeat(N)
    log_p_all = log_pX洧랚(g, X[idxX], 洧랚[idx洧랚]).reshape(N,N) # 洧랚s constant across rows, Xs constant across columns

    log_normalizers = torch.logsumexp(log_p_all, axis=-1)
    log_p = torch.diag(log_p_all)
    losses = log_p - log_normalizers

    v = (torch.eye(N) - torch.exp(log_p_all - log_normalizers.unsqueeze(-1))).unsqueeze(-1)  
    grad_log_p_all = grad_log_pX洧랚(g, X[idxX], 洧랚[idx洧랚]).reshape(N,N,-1)

    grads = (v * grad_log_p_all).sum(axis=-2)

    return grads.sum(axis=0), losses.sum(axis=0)
