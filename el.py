import numpy as np
import torch
import scipy.optimize as spo
from scipy.special import logsumexp

dtype = torch.float64
dtype_np = np.float64

def w_opt(ğœ†, G):
    ğœ‚ = torch.bmm(G, ğœ†.unsqueeze(-1)).squeeze(-1)
    w_inv =  T * (1. + ğœ‚)
    assert np.all(w_inv > 1.) # check if ğœ† in domain !
    return 1. / w_inv

def F(ğœ†, G):
    w = w_opt(ğœ†, G)
    return torch.bmm(w.unsqueeze(-2), G).squeeze(-2)

def F_np(ğœ†, G):
    w = w_opt(ğœ†, G)
    return torch.bmm(w.unsqueeze(-2), G).squeeze(-2)

def comp_dFdğœ†(ğœ†, G):

    w = w_opt(ğœ†, G)
    T = w.shape[-1]
    GTdiagw2 = (G  * (w**2).unsqueeze(-1)).transpose(-1,-2)
    dFdğœ† = - T * torch.bmm(GTdiagw2, G)

    return dFdğœ†

def solve_ğœ†(G):
    with torch.no_grad():
        N, K = G.shape[0], G.shape[-1]
        ğœ†0 = np.zeros((N, K))
        ğœ† = np.zeros_like(ğœ†0)
        for i in range(N):
            def F_G(ğœ†):
                return F_np(ğœ†, G[i].numpy())
            def gradF_G(ğœ†):
                return comp_dFdğœ†(ğœ†, G[i].numpy())
            ğœ†[i] = spo.newton(F_G, ğœ†0[i], fprime=gradF_G)['x']
    return torch.tensor(ğœ†,dtype=dtype)

def log_pXğœƒ(g, X, ğœƒ, eps=1e-4):
    pass
    return log_p

def comp_dFdğœ†(ğœ†, G, GTdiagw):

    pass

    return dFdğœ†

def comp_dFdğœ™(ğœ†, G, w, GTw, GTdiagw, dGdğœ™):

    pass

    return dFdğœ™

def grad_log_pXğœƒ(g, X, ğœƒ, eps=1e-4):

    pass

    return grad

def loss_InfoNCE(g, X, ğœƒ):

    pass

    return (log_p - log_normalizers).sum(axis=0)

def grad_InfoNCE(g, X, ğœƒ):

    pass

    return grads.sum(axis=0), losses.sum(axis=0)
