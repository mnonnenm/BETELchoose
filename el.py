import numpy as np
import torch
import scipy.optimize as spo
from scipy.optimize import fsolve
from etel import log_w_opt, Î¦_np

dtype = torch.float64
dtype_np = np.float64

def w_opt(ğœ†, G):

    ğœ‚ = torch.bmm(G, ğœ†.unsqueeze(-1)).squeeze(-1)
    T = ğœ‚.shape[-1]
    w_inv =  T * (1. + ğœ‚)
    #if torch.any(w_inv < 1.): # check if ğœ† in domain !
    #    print('warning: invalid w !')

    return 1. / w_inv

def F(ğœ†, G):

    w = w_opt(ğœ†, G)

    return torch.bmm(w.unsqueeze(-2), G).squeeze(-2)

def comp_dFdğœ†(ğœ†, G, GTdiagw2=None):

    w = w_opt(ğœ†, G)
    T = w.shape[-1]
    GTdiagw2 = (G  * (w**2).unsqueeze(-1)).transpose(-1,-2) if GTdiagw2 is None else GTdiagw2
    dFdğœ† = - T * torch.bmm(GTdiagw2, G)

    return dFdğœ†

def solve_ğœ†_ETEL(G):
    # We need ğœ†'g(x[i],ğœƒ) >= 1/T -1, or else the weights w(ğœ†,ğœƒ) can be negative or larger than 1 !
    # To avoid Lagrange multipliers on Lagrange multipliers ğœ†, we first solve for w and then for ğœ†.
    # We use the ETEL code for finding w from an exponential family guaranteed to contain w.
    with torch.no_grad():
        N, T, K = G.shape
        assert T >= K
        ğœ†0 = np.zeros(K)
        ğœ†_ETEL = np.zeros((N,K))
        ğœ† = torch.zeros((N,K))
        for i in range(N):
            def Î¦_G(ğœ†):
                return Î¦_np(ğœ†, G[i].numpy())
            ğœ†_ETEL[i] = spo.minimize(Î¦_G, ğœ†0)['x']
        w = torch.exp(log_w_opt(torch.tensor(ğœ†_ETEL,dtype=dtype), G))
        idx_K = torch.argsort(w, axis=1)[:,-K:]
        for i in range(N):
            ğœ†[i] = torch.linalg.solve(G[i,idx_K[i]], 1./(T*w[i,idx_K[i]]) - 1.)
    return ğœ†

def solve_ğœ†(G):
    # this version ignores the issue that the Lagrange multipliers themselves have linear constraints !
    # We need ğœ†'g(x[i],ğœƒ) >= 1/T -1, or else the weights w(ğœ†,ğœƒ) can be negative or larger than 1.
    # We avoid this by initializating with a very round-about call to similar ETEL code.
    with torch.no_grad():
        N, K = G.shape[0], G.shape[-1]
        ğœ†0 = solve_ğœ†_ETEL(G).detach().numpy()
        ğœ† = torch.zeros((N, K))
        for i in range(N):
            def F_G(ğœ†):
                return F(torch.tensor(ğœ†,dtype=dtype).unsqueeze(0), G[i].unsqueeze(0)).numpy().squeeze(0)
            def jacF_G(ğœ†):
                J = comp_dFdğœ†(torch.tensor(ğœ†,dtype=dtype).unsqueeze(0), G[i].unsqueeze(0))
                return J.numpy().squeeze(0)
            ğœ†[i] = torch.tensor(fsolve(F_G, ğœ†0[i], fprime=jacF_G), dtype=dtype)

    #assert torch.all(1./w_opt(ğœ†, G) >= 1.0) # assure ğœ† is in valid domain: 0 <= w[t] <= 1 for all t
    return ğœ†

def log_pXğœƒ(g, X, ğœƒ, eps=1e-4):

    G = g(X, ğœƒ)
    ğœ† = solve_ğœ†(G)
    w = w_opt(ğœ†, G)
    log_p = torch.log(w).sum(axis=-1)

    # check for estimating equation constraints to hold
    GTw = torch.bmm(w.unsqueeze(-2), G).transpose(-1,-2)
    idx_n_bad = torch.any(1./w_opt(ğœ†, G) < 1.0, axis=-1) # invalid w resp. ğœ†
    log_p[idx_n_bad] = - torch.inf
    idx_n_bad = torch.abs(GTw.squeeze(-1)).mean(axis=-1) >= eps # moment failed
    log_p[idx_n_bad] = - torch.inf
    
    return log_p

def comp_dFdğœ™(ğœ†, w, GTdiagw2, dGdğœ™):

    dFdğœ™ = {}
    T = w.shape[-1]
    for (p, dGdp) in dGdğœ™.items():
        ğœ†TdGdp = (dGdp.flatten(3) * ğœ†.unsqueeze(-2).unsqueeze(-1)).sum(axis=-2) # sum over K
        wTdGdp = (dGdp.flatten(3) * w.unsqueeze(-1).unsqueeze(-1)).sum(axis=1)  # sum over T
        dFdğœ™[p] = wTdGdp/T  - torch.bmm(GTdiagw2, ğœ†TdGdp)

    return dFdğœ™

def grad_log_pXğœƒ(g, X, ğœƒ, eps=1e-4):

    N,T = X.shape[:2]  # N x T x D
    G = g(X, ğœƒ)        # N x T x K
    ğœ† = solve_ğœ†(G)     # N     x K  
    w = w_opt(ğœ†, G)    # N x T

    GTw = torch.bmm(w.unsqueeze(-2), G).transpose(-1,-2)
    idx_n_good = torch.abs(GTw.squeeze(-1)).mean(axis=-1) < eps
    if idx_n_good.sum() < N:
        print(f"warning, {N - idx_n_good.sum()} out of {N} datapoints have zero likelihood and no gradient")
        X, ğœƒ, G, ğœ†, w, GTw = X[idx_n_good], ğœƒ[idx_n_good], G[idx_n_good], ğœ†[idx_n_good], w[idx_n_good], GTw[idx_n_good]

    GTdiagw2 = (G  * (w**2).unsqueeze(-1)).transpose(-1,-2)
    dGdğœ™ = g.jacobian_pars(X, ğœƒ)                    # struct of { par : N x T x K x dim(par) }

    # inverse function theorem 
    dFdğœ† = comp_dFdğœ†(ğœ†, G, GTdiagw2)         # N     x K x K
    dFdğœ™ = comp_dFdğœ™(ğœ†, w, GTdiagw2, dGdğœ™)  # struct of { par : N x K x dim(par) }
    dğœ†dğœ™ = {}                                # struct of { par : N x K x dim(par) }
    for (p, dFdp) in dFdğœ™.items():
        dğœ†dğœ™[p] = - torch.linalg.solve(dFdğœ†, dFdp) # Inverse function theorem: dğœ†dğœ™ = inv(dFdğœ†) * dFdğœ™ 

    # differentiating w* and g_ğœ™(X,ğœƒ) wrt ğœ™        
    diff  = (1. - T * w).unsqueeze(-1)              # N x T x 1
    grad = {}                                       # struct of { par : N x T x dim(par) }
    for (p, dGdp) in dGdğœ™.items():
        grad[p] = torch.zeros((N, np.prod(dGdp.shape[3:]))) # N x dim(par)
        grad[p][idx_n_good] = torch.bmm(ğœ†.unsqueeze(-2), (diff.unsqueeze(-1) * dGdp.flatten(3)).sum(axis=1)).squeeze(-2)
    for (p, dğœ†dp) in dğœ†dğœ™.items():
        grad[p][idx_n_good] = grad[p][idx_n_good] + torch.bmm((diff * G).sum(axis=-2).unsqueeze(-2), dğœ†dp).squeeze(-2)

    return grad

def loss_InfoNCE(g, X, ğœƒ):

    pass

    return (log_p - log_normalizers).sum(axis=0)

def grad_InfoNCE(g, X, ğœƒ):

    pass

    return grads.sum(axis=0), losses.sum(axis=0)
