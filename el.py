import numpy as np
import torch
import scipy.optimize as spo
from scipy.optimize import fsolve
from etel import log_w_opt, Î¦_np

dtype = torch.float64
dtype_np = np.float64

def w_opt(ğœ†, G):

    ğœ‚ = torch.bmm(G, ğœ†.unsqueeze(-1)).squeeze(-1)
    T = ğœ‚.shape[-1]
    w_inv =  T * (1. + ğœ‚)
    #if torch.any(w_inv < 1.): # check if ğœ† in domain !
    #    print('warning: invalid w !')

    return 1. / w_inv

def F(ğœ†, G):

    w = w_opt(ğœ†, G)

    return torch.bmm(w.unsqueeze(-2), G).squeeze(-2)

def comp_dFdğœ†(ğœ†, G):

    w = w_opt(ğœ†, G)
    T = w.shape[-1]
    GTdiagw2 = (G  * (w**2).unsqueeze(-1)).transpose(-1,-2)
    dFdğœ† = - T * torch.bmm(GTdiagw2, G)

    return dFdğœ†

def solve_ğœ†_ETEL(G):
    # We need ğœ†'g(x[i],ğœƒ) >= 1/T -1, or else the weights w(ğœ†,ğœƒ) can be negative or larger than 1 !
    # To avoid Lagrange multipliers on Lagrange multipliers ğœ†, we first solve for w and then for ğœ†.
    # We use the ETEL code for finding w from an exponential family guaranteed to contain w.
    with torch.no_grad():
        N, T, K = G.shape
        assert T >= K
        ğœ†0 = np.zeros(K)
        ğœ†_ETEL = np.zeros((N,K))
        ğœ† = torch.zeros((N,K))
        for i in range(N):
            def Î¦_G(ğœ†):
                return Î¦_np(ğœ†, G[i].numpy())
            ğœ†_ETEL[i] = spo.minimize(Î¦_G, ğœ†0)['x']
        w = torch.exp(log_w_opt(torch.tensor(ğœ†_ETEL,dtype=dtype), G))
        idx_K = torch.argsort(w, axis=1)[:,-K:]
        for i in range(N):
            ğœ†[i] = torch.linalg.solve(G[i,idx_K[i]], 1./(T*w[i,idx_K[i]]) - 1.)
    return ğœ†

def solve_ğœ†(G):
    # this version ignores the issue that the Lagrange multipliers themselves have linear constraints !
    # We need ğœ†'g(x[i],ğœƒ) >= 1/T -1, or else the weights w(ğœ†,ğœƒ) can be negative or larger than 1.
    # We avoid this by initializating with a very round-about call to similar ETEL code.
    with torch.no_grad():
        N, K = G.shape[0], G.shape[-1]
        ğœ†0 = solve_ğœ†_ETEL(G).detach().numpy()
        ğœ† = torch.zeros((N, K))
        for i in range(N):
            def F_G(ğœ†):
                return F(torch.tensor(ğœ†,dtype=dtype).unsqueeze(0), G[i].unsqueeze(0)).numpy().squeeze(0)
            def jacF_G(ğœ†):
                J = comp_dFdğœ†(torch.tensor(ğœ†,dtype=dtype).unsqueeze(0), G[i].unsqueeze(0))
                return J.numpy().squeeze(0)
            ğœ†[i] = torch.tensor(fsolve(F_G, ğœ†0[i], fprime=jacF_G), dtype=dtype)

    #assert torch.all(1./w_opt(ğœ†, G) >= 1.0) # assure ğœ† is in valid domain: 0 <= w[t] <= 1 for all t
    return ğœ†

def log_pXğœƒ(g, X, ğœƒ, eps=1e-4):

    G = g(X, ğœƒ)
    ğœ† = solve_ğœ†(G)
    w = w_opt(ğœ†, G)
    log_p = torch.log(w).sum(axis=-1)

    # check for estimating equation constraints to hold
    GTw = torch.bmm(w.unsqueeze(-2), G).transpose(-1,-2)
    idx_n_bad = torch.any(1./w_opt(ğœ†, G) < 1.0, axis=-1) # invalid w resp. ğœ†
    log_p[idx_n_bad] = - torch.inf
    idx_n_bad = torch.abs(GTw.squeeze(-1)).mean(axis=-1) >= eps # moment failed
    log_p[idx_n_bad] = - torch.inf
    
    return log_p

def comp_dFdğœ™(ğœ†, G, w, GTw, GTdiagw, dGdğœ™):

    pass

    return dFdğœ™

def grad_log_pXğœƒ(g, X, ğœƒ, eps=1e-4):

    pass

    return grad

def loss_InfoNCE(g, X, ğœƒ):

    pass

    return (log_p - log_normalizers).sum(axis=0)

def grad_InfoNCE(g, X, ğœƒ):

    pass

    return grads.sum(axis=0), losses.sum(axis=0)
